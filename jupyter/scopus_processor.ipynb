{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных Scopus для Khazar University\n",
    "\n",
    "Этот notebook извлекает новые статьи из экспорта Scopus, которых еще нет в базе United Scopus and WoS.\n",
    "\n",
    "**Алгоритм:**\n",
    "1. Загрузить исходный файл Scopus и целевой файл United\n",
    "2. Найти статьи, которых нет в United (fuzzy matching по названию)\n",
    "3. Извлечь авторов, аффилированных с Khazar University\n",
    "4. Сопоставить департаменты по справочнику\n",
    "5. Сформировать выходной файл с подсветкой проблемных ячеек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Конфигурация\n",
    "\n",
    "**Измените пути к файлам здесь для обработки новых данных**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:44.934782Z",
     "start_time": "2025-11-04T13:39:44.927924Z"
    }
   },
   "source": [
    "# ==================== НАСТРОЙКИ ====================\n",
    "\n",
    "# Пути к файлам\n",
    "SCOPUS_SOURCE_FILE = '../data/scopus_export_Nov 3-2025_source.xlsx'\n",
    "UNITED_TARGET_FILE = '../data/United_Scopus and WoS_2024.xlsx'\n",
    "UNITED_SHEET_NAME = 'Last'  # Название листа в United файле для чтения\n",
    "DEPARTMENTS_MAPPING_FILE = '../data/departments_mapping.xlsx'\n",
    "OUTPUT_FILE = 'data/output/new_articles_{year}_{date}.xlsx'  # {date} будет заменен на текущую дату\n",
    "\n",
    "# Настройки поиска\n",
    "FUZZY_MATCH_THRESHOLD = 95  # Порог совпадения для fuzzy matching (0-100)\n",
    "KHAZAR_KEYWORDS = [\n",
    "    'Khazar University',\n",
    "    'Khazar',\n",
    "    'Xəzər Universiteti'\n",
    "]\n",
    "# Год(ы) для фильтрации статей\n",
    "# Варианты:\n",
    "#   YEAR = 2025              # один год\n",
    "#   YEAR = [2024, 2025]      # несколько лет\n",
    "#   YEAR = None              # все годы (без фильтрации)\n",
    "YEAR = [2025, 2026]\n",
    "\n",
    "# Фильтрация по Title - исключение статей\n",
    "# Если Title содержит любую из подстрок (регистр не важен), статья будет исключена\n",
    "# Пример: ['Correction', 'Erratum', 'Retracted', 'Withdrawn']\n",
    "TITLE_EXCLUDE_KEYWORDS = ['Correction:', 'Correction to:', 'Erratum to', 'Corrigendum to']\n",
    "\n",
    "# Настройки форматирования\n",
    "HIGHLIGHT_COLOR_MULTIPLE_DEPTS = 'FFFF00'  # Желтый для множественных департаментов\n",
    "HIGHLIGHT_COLOR_NO_DEPT = 'FFFFFF'  # Белый для отсутствующих департаментов\n",
    "\n",
    "print('✓ Конфигурация загружена')\n",
    "print(f'  Исходный файл: {SCOPUS_SOURCE_FILE}')\n",
    "print(f'  Целевой файл: {UNITED_TARGET_FILE}')\n",
    "print(f'  Лист в United файле: {UNITED_SHEET_NAME}')\n",
    "print(f'  Справочник департаментов: {DEPARTMENTS_MAPPING_FILE}')\n",
    "print(f'  Порог fuzzy matching: {FUZZY_MATCH_THRESHOLD}%')\n",
    "print(f'  Фильтрация по году: {YEAR}')\n",
    "print(f'  Исключаемые подстроки в Title: {TITLE_EXCLUDE_KEYWORDS if TITLE_EXCLUDE_KEYWORDS else \"нет\"}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Конфигурация загружена\n",
      "  Исходный файл: data/scopus_export_Nov 3-2025_source.xlsx\n",
      "  Целевой файл: data/United_Scopus and WoS_2024.xlsx\n",
      "  Лист в United файле: Last\n",
      "  Справочник департаментов: data/departments_mapping.xlsx\n",
      "  Порог fuzzy matching: 95%\n",
      "  Фильтрация по году: [2025, 2026]\n",
      "  Исключаемые подстроки в Title: ['Correction:', 'Correction to:', 'Erratum to']\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Импорт библиотек",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Основные библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Для работы с Excel\n",
    "import openpyxl\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "# Для fuzzy matching\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Настройки отображения pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print('✓ Библиотеки импортированы успешно')\n",
    "print(f'  pandas: {pd.__version__}')\n",
    "print(f'  openpyxl: {openpyxl.__version__}')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:44.943097Z",
     "start_time": "2025-11-04T13:39:44.939756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Библиотеки импортированы успешно\n",
      "  pandas: 2.3.3\n",
      "  openpyxl: 3.1.5\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Загрузка данных",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print('Загрузка файлов...\\n')\n\n# Загрузка исходного файла Scopus\nprint(f'Загружаем: {SCOPUS_SOURCE_FILE}')\ndf_scopus = pd.read_excel(SCOPUS_SOURCE_FILE)\nprint(f'  ✓ Загружено статей: {len(df_scopus)}')\nprint(f'  ✓ Колонок: {len(df_scopus.columns)}')\n\n# Загрузка целевого файла United\nprint(f'\\nЗагружаем: {UNITED_TARGET_FILE}')\nprint(f'  Лист: {UNITED_SHEET_NAME}')\ndf_united = pd.read_excel(UNITED_TARGET_FILE, sheet_name=UNITED_SHEET_NAME)\nprint(f'  ✓ Загружено статей: {len(df_united)}')\nprint(f'  ✓ Колонок: {len(df_united.columns)}')\n\n# Загрузка справочника департаментов (если существует)\nprint(f'\\nЗагружаем: {DEPARTMENTS_MAPPING_FILE}')\nif os.path.exists(DEPARTMENTS_MAPPING_FILE):\n    df_departments = pd.read_excel(DEPARTMENTS_MAPPING_FILE)\n    print(f'  ✓ Загружено записей: {len(df_departments)}')\nelse:\n    print('  ⚠ Файл не найден. Будет создан пустой справочник.')\n    df_departments = pd.DataFrame(columns=['Author Name', 'Departament'])\n\nprint('\\n' + '='*70)\nprint('Просмотр структуры данных:')\nprint('='*70)\nprint('\\n--- Scopus файл (3 случайные строки Title + Authors) ---')\nprint(df_scopus[['Title', 'Authors']].sample(3))\nprint('\\n--- United файл (3 случайные строки Title + Authors) ---')\nprint(df_united[['Title', 'Authors']].sample(3))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:45.287574Z",
     "start_time": "2025-11-04T13:39:44.949585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка файлов...\n",
      "\n",
      "Загружаем: data/scopus_export_Nov 3-2025_source.xlsx\n",
      "  ✓ Загружено статей: 1334\n",
      "  ✓ Колонок: 22\n",
      "\n",
      "Загружаем: data/United_Scopus and WoS_2024.xlsx\n",
      "  Лист: Last\n",
      "  ✓ Загружено статей: 1684\n",
      "  ✓ Колонок: 19\n",
      "\n",
      "Загружаем: data/departments_mapping.xlsx\n",
      "  ✓ Загружено записей: 2\n",
      "\n",
      "======================================================================\n",
      "Просмотр структуры данных:\n",
      "======================================================================\n",
      "\n",
      "--- Scopus файл (3 случайные строки Title + Authors) ---\n",
      "                                                                                                   Title                                                                                              Authors\n",
      "613                                Two-body Dirac equation in DSR: Results for fermion-antifermion pairs                                                                              Jafari, N.; Guvendi, A.\n",
      "600  Numerical investigation on the performance of the solar air collector using jet impingement with...  Afridi, M.I.; Samarmad, A.O.; Ali, M.; Rashid, F.L.; Kadhim, S.A.; Jasim, A.K.; Nayyef, D.R.; Ba...\n",
      "108               Dynamic instability analysis of piezoelectric nanoplates under combined AC/DC voltages  Basem, A.B.M.; Al-Nussairi, A.K.J.; Sawaran Singh, N.S.S.; Hashim, A.M.; Salahshour, S.; Sajadi,...\n",
      "\n",
      "--- United файл (3 случайные строки Title + Authors) ---\n",
      "                                                                                                   Title                                  Authors\n",
      "360                                   The Resolvent of Impulsive Singular Hahn–Sturm–Liouville Operators  Allahverdiev B.P.; Tuna H.; Isayev H.A.\n",
      "518  Exploring the Enigma of Particle Dynamics and Plasma Lensing Using Einstein–Euler–Heisenberg Bla...                             Ditta, Allah\n",
      "437                                    Feature-weighted fuzzy clustering methods: An experimental review                          Arasteh, Bahman\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Функции обработки\n\n### 4.1 Поиск новых статей (fuzzy matching)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_title(title):\n",
    "    \"\"\"\n",
    "    Нормализует название статьи для сравнения.\n",
    "    Убирает лишние пробелы, приводит к нижнему регистру.\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', str(title).lower().strip())\n",
    "\n",
    "\n",
    "def find_new_articles(df_source, df_existing, threshold=95):\n",
    "    \"\"\"\n",
    "    Находит статьи из df_source, которых нет в df_existing.\n",
    "    \n",
    "    Параметры:\n",
    "    - df_source: DataFrame с исходными статьями (Scopus)\n",
    "    - df_existing: DataFrame с существующими статьями (United)\n",
    "    - threshold: порог fuzzy matching (0-100)\n",
    "    \n",
    "    Возвращает:\n",
    "    - DataFrame с новыми статьями\n",
    "    - dict с информацией о найденных дубликатах\n",
    "    \"\"\"\n",
    "    print(f'Поиск новых статей (порог fuzzy matching: {threshold}%)...\\n')\n",
    "    \n",
    "    # Нормализуем названия в существующих статьях\n",
    "    existing_titles = df_existing['Title'].apply(normalize_title).tolist()\n",
    "    \n",
    "    new_articles = []\n",
    "    duplicates_info = []\n",
    "    \n",
    "    for idx, row in df_source.iterrows():\n",
    "        source_title = normalize_title(row['Title'])\n",
    "        \n",
    "        # Проверяем на дубликаты с fuzzy matching\n",
    "        is_duplicate = False\n",
    "        best_match_score = 0\n",
    "        best_match_title = \"\"\n",
    "        \n",
    "        for existing_title in existing_titles:\n",
    "            similarity = fuzz.ratio(source_title, existing_title)\n",
    "            \n",
    "            if similarity > best_match_score:\n",
    "                best_match_score = similarity\n",
    "                best_match_title = existing_title\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                is_duplicate = True\n",
    "\n",
    "                duplicates_info.append({\n",
    "                    'source_title': row['Title'],\n",
    "                    'matched_title': existing_title,\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "                break\n",
    "        \n",
    "        # Если не дубликат, добавляем в список новых\n",
    "        if not is_duplicate:\n",
    "            new_articles.append(idx)\n",
    "        \n",
    "        # Прогресс\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f'  Обработано: {idx + 1}/{len(df_source)} статей')\n",
    "    \n",
    "    df_new = df_source.loc[new_articles].copy()\n",
    "    \n",
    "    print(f'\\n✓ Анализ завершен:')\n",
    "    print(f'  Всего статей в источнике: {len(df_source)}')\n",
    "    print(f'  Найдено дубликатов: {len(duplicates_info)}')\n",
    "    print(f'  Новых статей: {len(df_new)}')\n",
    "    \n",
    "    return df_new, duplicates_info\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:45.293415Z",
     "start_time": "2025-11-04T13:39:45.290067Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Парсинг авторов из Khazar University",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_khazar_authors(authors_with_affiliations, author_full_names, keywords=KHAZAR_KEYWORDS):\n",
    "    \"\"\"\n",
    "    Извлекает авторов из строки с аффилиациями, в именах которых есть слова из `keywords`.\n",
    "    \n",
    "    Параметры:\n",
    "    - authors_with_affiliations: строка \"LastName, FirstName, affiliation; LastName, FirstName, affiliation; ...\"\n",
    "    - author_full_names: строка с полными именами и ID \"Name (ID); Name (ID); ...\"\n",
    "    - keywords: список ключевых слов для поиска университета\n",
    "    \n",
    "    Возвращает:\n",
    "    - dict с информацией об авторах Khazar:\n",
    "        {\n",
    "            'authors_short': 'LastName, F.; LastName2, F.',  # Краткие имена\n",
    "            'authors_with_ids': 'Full Name (ID); Full Name2 (ID)',  # С ID\n",
    "            'authors_full': 'Full Name; Full Name2',  # Полные имена\n",
    "            'count': 2  # Количество авторов\n",
    "        }\n",
    "    \"\"\"\n",
    "    if pd.isna(authors_with_affiliations):\n",
    "        return {'authors_short': '', 'authors_with_ids': '', 'authors_full': '', 'count': 0}\n",
    "    \n",
    "    # Разбиваем на отдельных авторов (разделитель - точка с запятой)\n",
    "    author_blocks = str(authors_with_affiliations).split(';')\n",
    "    \n",
    "    khazar_authors_short = []\n",
    "    khazar_authors_with_ids = []\n",
    "    khazar_authors_full = []\n",
    "    \n",
    "    # Парсим author_full_names для получения ID\n",
    "    full_names_dict = {}\n",
    "    if not pd.isna(author_full_names):\n",
    "        # Формат: \"LastName, FirstName (ID); LastName, FirstName (ID); ...\"\n",
    "        full_name_parts = str(author_full_names).split(';')\n",
    "        for part in full_name_parts:\n",
    "            part = part.strip()\n",
    "            # Извлекаем имя и ID\n",
    "            match = re.match(r'(.+?)\\s*\\((\\d+)\\)', part)\n",
    "            if match:\n",
    "                name = match.group(1).strip()\n",
    "                author_id = match.group(2).strip()\n",
    "                # Создаем ключ по фамилии для поиска\n",
    "                name_parts = name.split(',')\n",
    "                if len(name_parts) >= 1:\n",
    "                    last_name = name_parts[0].strip()\n",
    "                    full_names_dict[last_name] = {'full': name, 'id': author_id}\n",
    "    \n",
    "    # Проверяем каждого автора\n",
    "    for block in author_blocks:\n",
    "        block = block.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "        \n",
    "        # Проверяем, содержит ли блок ключевые слова Khazar\n",
    "        is_khazar = any(keyword.lower() in block.lower() for keyword in keywords)\n",
    "        \n",
    "        if is_khazar:\n",
    "            # Извлекаем имя автора (формат: \"LastName, FirstName, affiliation, affiliation, ...\")\n",
    "            # Имя автора - это первые две части до запятых\n",
    "            parts = block.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                last_name = parts[0].strip()\n",
    "                first_name = parts[1].strip()\n",
    "                \n",
    "                # Краткое имя: \"LastName, F.\"\n",
    "                first_initial = first_name[0] + '.' if first_name else ''\n",
    "                short_name = f\"{last_name}, {first_initial}\"\n",
    "                khazar_authors_short.append(short_name)\n",
    "                \n",
    "                # Полное имя с ID из author_full_names\n",
    "                if last_name in full_names_dict:\n",
    "                    full_info = full_names_dict[last_name]\n",
    "                    khazar_authors_with_ids.append(f\"{full_info['full']} ({full_info['id']})\")\n",
    "                    khazar_authors_full.append(full_info['full'])\n",
    "                else:\n",
    "                    # Если ID не найден, используем имя без ID\n",
    "                    full_name = f\"{last_name}, {first_name}\"\n",
    "                    khazar_authors_with_ids.append(full_name)\n",
    "                    khazar_authors_full.append(full_name)\n",
    "    \n",
    "    return {\n",
    "        'authors_short': '; '.join(khazar_authors_short),\n",
    "        'authors_with_ids': '; '.join(khazar_authors_with_ids),\n",
    "        'authors_full': '; '.join(khazar_authors_full),\n",
    "        'count': len(khazar_authors_short)\n",
    "    }"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:45.299252Z",
     "start_time": "2025-11-04T13:39:45.295514Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 Сопоставление департаментов",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def map_departments(authors_string, df_dept_mapping):\n",
    "    \"\"\"\n",
    "    Сопоставляет авторов с департаментами из справочника.\n",
    "    \n",
    "    Параметры:\n",
    "    - authors_string: строка с авторами, разделенными \"; \" (например, \"LastName, F.; LastName2, F.\")\n",
    "    - df_dept_mapping: DataFrame со справочником (колонки: 'Author Name', 'Departament')\n",
    "    \n",
    "    Возвращает:\n",
    "    - dict с информацией:\n",
    "        {\n",
    "            'department': строка с департаментом(ами) через \"; \"\n",
    "            'needs_highlight': bool - нужна ли желтая подсветка\n",
    "            'reason': причина подсветки ('not_found', 'multiple', None)\n",
    "        }\n",
    "    \"\"\"\n",
    "    if pd.isna(authors_string) or not authors_string.strip():\n",
    "        return {'department': '', 'needs_highlight': False, 'reason': None}\n",
    "    \n",
    "    # Разбиваем на отдельных авторов\n",
    "    authors = [a.strip() for a in str(authors_string).split(';') if a.strip()]\n",
    "    \n",
    "    departments = []\n",
    "    not_found_authors = []\n",
    "    \n",
    "    for author in authors:\n",
    "        # Ищем автора в справочнике (нечувствительно к регистру)\n",
    "        matches = df_dept_mapping[\n",
    "            df_dept_mapping['Author Name'].str.lower() == author.lower()\n",
    "        ]\n",
    "        \n",
    "        if len(matches) == 0:\n",
    "            # Автор не найден\n",
    "            not_found_authors.append(author)\n",
    "        else:\n",
    "            # Автор найден, извлекаем департамент(ы)\n",
    "            for _, row in matches.iterrows():\n",
    "                dept = row['Departament']\n",
    "                if pd.notna(dept) and dept.strip():\n",
    "                    departments.append(dept.strip())\n",
    "    \n",
    "    # Убираем дубликаты департаментов\n",
    "    departments = list(dict.fromkeys(departments))  # Сохраняет порядок\n",
    "    \n",
    "    # Определяем нужна ли подсветка\n",
    "    needs_highlight = False\n",
    "    reason = None\n",
    "\n",
    "    if not_found_authors:\n",
    "        # Есть авторы без департаментов\n",
    "        needs_highlight = True\n",
    "        reason = 'not_found'\n",
    "    elif len(departments) > 1:\n",
    "        # Множественные департаменты\n",
    "        needs_highlight = True\n",
    "        reason = 'multiple'\n",
    "\n",
    "    department_str = '; '.join(departments) if departments else ''\n",
    "\n",
    "    return {\n",
    "        'department': department_str,\n",
    "        'needs_highlight': needs_highlight,\n",
    "        'reason': reason,\n",
    "        'not_found_authors': not_found_authors\n",
    "    }\n",
    "\n",
    "\n",
    "# Тестируем функцию\n",
    "print('='*70)\n",
    "print('ТЕСТ: Сопоставление департаментов')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "# Создаем тестовый справочник\n",
    "test_dept_df = pd.DataFrame({\n",
    "    'Author Name': ['Mahmood, S.', 'Jafari, N.', 'Ali, A.'],\n",
    "    'Departament': [\n",
    "        'Aşağı ölçülü materialların tədqiqat mərkəzi',\n",
    "        'Center for Theoretical Physics',\n",
    "        'Kimya və kimya mühəndisliyi'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('Справочник департаментов:')\n",
    "print(test_dept_df)\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "\n",
    "# Тест 1: Автор найден\n",
    "print('\\nТест 1: Один автор найден в справочнике')\n",
    "result1 = map_departments('Mahmood, S.', test_dept_df)\n",
    "print(f\"Автор: Mahmood, S.\")\n",
    "print(f\"Департамент: {result1['department']}\")\n",
    "print(f\"Нужна подсветка: {result1['needs_highlight']} ({result1['reason']})\")\n",
    "\n",
    "# Тест 2: Автор не найден\n",
    "print('\\nТест 2: Автор не найден в справочнике')\n",
    "result2 = map_departments('Unknown, U.', test_dept_df)\n",
    "print(f\"Автор: Unknown, U.\")\n",
    "print(f\"Департамент: {result2['department']}\")\n",
    "print(f\"Нужна подсветка: {result2['needs_highlight']} ({result2['reason']})\")\n",
    "print(f\"Авторы без департамента: {result2['not_found_authors']}\")\n",
    "\n",
    "# Тест 3: Множественные авторы с разными департаментами\n",
    "print('\\nТест 3: Несколько авторов с разными департаментами')\n",
    "result3 = map_departments('Mahmood, S.; Jafari, N.', test_dept_df)\n",
    "print(f\"Авторы: Mahmood, S.; Jafari, N.\")\n",
    "print(f\"Департаменты: {result3['department']}\")\n",
    "print(f\"Нужна подсветка: {result3['needs_highlight']} ({result3['reason']})\")\n",
    "\n",
    "# Тест 4: Смешанный случай (один найден, один нет)\n",
    "print('\\nТест 4: Один найден, один нет')\n",
    "result4 = map_departments('Mahmood, S.; Unknown, U.', test_dept_df)\n",
    "print(f\"Авторы: Mahmood, S.; Unknown, U.\")\n",
    "print(f\"Департаменты: {result4['department']}\")\n",
    "print(f\"Нужна подсветка: {result4['needs_highlight']} ({result4['reason']})\")\n",
    "print(f\"Авторы без департамента: {result4['not_found_authors']}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:45.308412Z",
     "start_time": "2025-11-04T13:39:45.301239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ТЕСТ: Сопоставление департаментов\n",
      "======================================================================\n",
      "\n",
      "Справочник департаментов:\n",
      "   Author Name                                  Departament\n",
      "0  Mahmood, S.  Aşağı ölçülü materialların tədqiqat mərkəzi\n",
      "1   Jafari, N.               Center for Theoretical Physics\n",
      "2      Ali, A.                  Kimya və kimya mühəndisliyi\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Тест 1: Один автор найден в справочнике\n",
      "Автор: Mahmood, S.\n",
      "Департамент: Aşağı ölçülü materialların tədqiqat mərkəzi\n",
      "Нужна подсветка: False (None)\n",
      "\n",
      "Тест 2: Автор не найден в справочнике\n",
      "Автор: Unknown, U.\n",
      "Департамент: \n",
      "Нужна подсветка: True (not_found)\n",
      "Авторы без департамента: ['Unknown, U.']\n",
      "\n",
      "Тест 3: Несколько авторов с разными департаментами\n",
      "Авторы: Mahmood, S.; Jafari, N.\n",
      "Департаменты: Aşağı ölçülü materialların tədqiqat mərkəzi; Center for Theoretical Physics\n",
      "Нужна подсветка: True (multiple)\n",
      "\n",
      "Тест 4: Один найден, один нет\n",
      "Авторы: Mahmood, S.; Unknown, U.\n",
      "Департаменты: Aşağı ölçülü materialların tədqiqat mərkəzi\n",
      "Нужна подсветка: True (not_found)\n",
      "Авторы без департамента: ['Unknown, U.']\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Основной Pipeline обработки",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def process_scopus_data(df_source, df_existing, df_dept_mapping, threshold=95, year=None, title_exclude_keywords=None):\n",
    "    \"\"\"\n",
    "    Основной pipeline обработки данных Scopus.\n",
    "    \n",
    "    Параметры:\n",
    "    - df_source: DataFrame с исходными данными Scopus\n",
    "    - df_existing: DataFrame с существующими данными United\n",
    "    - df_dept_mapping: DataFrame со справочником департаментов\n",
    "    - threshold: порог fuzzy matching для поиска дубликатов\n",
    "    - year: год(ы) для фильтрации статей\n",
    "        None - без фильтрации (все годы)\n",
    "        int - один год (например, 2025)\n",
    "        list - несколько лет (например, [2024, 2025])\n",
    "    - title_exclude_keywords: список подстрок для исключения статей по Title (case-insensitive)\n",
    "    \n",
    "    Возвращает:\n",
    "    - df_result: DataFrame с новыми статьями в формате United\n",
    "    - stats: словарь со статистикой обработки\n",
    "    \"\"\"\n",
    "    \n",
    "    print('='*70)\n",
    "    print('ОСНОВНОЙ PIPELINE ОБРАБОТКИ')\n",
    "    print('='*70 + '\\n')\n",
    "\n",
    "    # Фильтрация по году\n",
    "    if year is not None:\n",
    "        # Приводим год к списку для единообразной обработки\n",
    "        years_list = [year] if isinstance(year, int) else year\n",
    "        \n",
    "        print(f'Фильтрация по году: {years_list}')\n",
    "\n",
    "        # Фильтруем Scopus\n",
    "        original_scopus_count = len(df_source)\n",
    "        df_source = df_source[df_source['Year'].isin(years_list)].copy()\n",
    "        print(f'  Scopus - всего статей: {original_scopus_count}')\n",
    "        print(f'  Scopus - статей за {years_list}: {len(df_source)}')\n",
    "\n",
    "        # Фильтруем United\n",
    "        original_united_count = len(df_existing)\n",
    "        df_existing = df_existing[df_existing['Year'].isin(years_list)].copy()\n",
    "        print(f'  United - всего статей: {original_united_count}')\n",
    "        print(f'  United - статей за {years_list}: {len(df_existing)}')\n",
    "        print()\n",
    "    \n",
    "    # Фильтрация по Title - исключение статей с определенными подстроками\n",
    "    if title_exclude_keywords and len(title_exclude_keywords) > 0:\n",
    "        print(f'Фильтрация по Title (исключение статей с подстроками):')\n",
    "        print(f'  Подстроки для исключения: {title_exclude_keywords}')\n",
    "        \n",
    "        before_filter_count = len(df_source)\n",
    "        \n",
    "        # Создаем маску: True если Title НЕ содержит ни одной из подстрок\n",
    "        def should_keep_article(title):\n",
    "            if pd.isna(title):\n",
    "                return True\n",
    "            title_lower = str(title).lower()\n",
    "            # Если хотя бы одна подстрока найдена - исключаем (return False)\n",
    "            for keyword in title_exclude_keywords:\n",
    "                if keyword.lower() in title_lower:\n",
    "                    print('*'*70+'\\n')\n",
    "                    print(f'keyword: {keyword}')\n",
    "                    print(f'title_lower: {title_lower}')\n",
    "                    return False\n",
    "            return True\n",
    "        \n",
    "        df_source = df_source[df_source['Title'].apply(should_keep_article)].copy()\n",
    "        \n",
    "        excluded_count = before_filter_count - len(df_source)\n",
    "        print(f'  Scopus - до фильтрации: {before_filter_count}')\n",
    "        print(f'  Scopus - исключено статей: {excluded_count}')\n",
    "        print(f'  Scopus - осталось статей: {len(df_source)}')\n",
    "        print()\n",
    "\n",
    "    # Шаг 1: Найти новые статьи (сравниваются только статьи за выбранный год)\n",
    "    df_new, duplicates = find_new_articles(df_source, df_existing, threshold)\n",
    "    \n",
    "    if len(df_new) == 0:\n",
    "        print('\\n⚠ Новых статей не найдено. Обработка завершена.')\n",
    "        return pd.DataFrame(), {'new_articles': 0, 'khazar_articles': 0, 'highlighted': 0}\n",
    "    \n",
    "    # Шаг 2: Обработать каждую новую статью\n",
    "    print('\\n' + '='*70)\n",
    "    print('Обработка новых статей...')\n",
    "    print('='*70 + '\\n')\n",
    "    \n",
    "    result_data = []\n",
    "    stats = {\n",
    "        'new_articles': len(df_new),\n",
    "        'khazar_articles': 0,\n",
    "        'highlighted_depts': 0,\n",
    "        'no_khazar_authors': 0\n",
    "    }\n",
    "    \n",
    "    for idx, (_, row) in enumerate(df_new.iterrows(), 1):\n",
    "        # Извлечь авторов Khazar\n",
    "        authors_info = extract_khazar_authors(\n",
    "            row['Authors with affiliations'],\n",
    "            row['Author full names']\n",
    "        )\n",
    "        \n",
    "        # Пропустить статьи без авторов Khazar\n",
    "        if authors_info['count'] == 0:\n",
    "            stats['no_khazar_authors'] += 1\n",
    "            continue\n",
    "        \n",
    "        stats['khazar_articles'] += 1\n",
    "        \n",
    "        # Сопоставить департаменты\n",
    "        dept_info = map_departments(authors_info['authors_short'], df_dept_mapping)\n",
    "        \n",
    "        if dept_info['needs_highlight']:\n",
    "            stats['highlighted_depts'] += 1\n",
    "        \n",
    "        # Сформировать запись\n",
    "        # ВАЖНО: Authors.1 и Author full names содержат ВСЕХ авторов из исходного Scopus файла\n",
    "        # Authors содержит только авторов Khazar (для согласованности с форматом United)\n",
    "        result_row = {\n",
    "            'Departament': dept_info['department'],\n",
    "            'Authors': authors_info['authors_short'],  # Только Khazar авторы (краткий формат)\n",
    "            'Authors.1': row['Authors'] if 'Authors' in row else '',  # ВСЕ авторы (краткий формат)\n",
    "            'Author full names': row['Author full names'] if 'Author full names' in row else '',  # ВСЕ полные имена с ID\n",
    "            'Title': row['Title'],\n",
    "            'Year': row['Year'] if 'Year' in row and pd.notna(row['Year']) else '',\n",
    "            'Source title': row['Source title'] if 'Source title' in row else '',\n",
    "            'Volume': row['Volume'] if 'Volume' in row else '',\n",
    "            'Issue': row['Issue'] if 'Issue' in row else '',\n",
    "            'Art. No.': row['Art. No.'] if 'Art. No.' in row else '',\n",
    "            'Page start': row['Page start'] if 'Page start' in row else '',\n",
    "            'Page end': row['Page end'] if 'Page end' in row else '',\n",
    "            'Page count': row['Page count'] if 'Page count' in row else '',\n",
    "            'Source': 'Scopus',\n",
    "            'Təqdimat': '',\n",
    "            'Data': '',\n",
    "            'Amount': '',\n",
    "            'Quartil': '',\n",
    "            '_highlight': dept_info['needs_highlight'],  # Флаг для подсветки\n",
    "            '_highlight_reason': dept_info['reason']\n",
    "        }\n",
    "        \n",
    "        result_data.append(result_row)\n",
    "        \n",
    "        # Прогресс\n",
    "        if idx % 10 == 0:\n",
    "            print(f'  Обработано: {idx}/{len(df_new)} статей')\n",
    "    \n",
    "    # Создать итоговый DataFrame\n",
    "    df_r = pd.DataFrame(result_data)\n",
    "    \n",
    "    # Статистика\n",
    "    print(f'\\n✓ Обработка завершена!')\n",
    "    print(f'\\n' + '='*70)\n",
    "    print('СТАТИСТИКА')\n",
    "    print('='*70)\n",
    "    print(f'  Всего новых статей найдено: {stats[\"new_articles\"]}')\n",
    "    print(f'  Статей с авторами Khazar: {stats[\"khazar_articles\"]}')\n",
    "    print(f'  Статей без авторов Khazar: {stats[\"no_khazar_authors\"]}')\n",
    "    print(f'  Статей с подсветкой департаментов: {stats[\"highlighted_depts\"]}')\n",
    "    print(f'  Итого строк в результате: {len(df_r)}')\n",
    "    \n",
    "    return df_r, stats\n",
    "\n",
    "\n",
    "# Запускаем основной pipeline\n",
    "print('\\n\\n')\n",
    "print('#'*70)\n",
    "print('# НАЧАЛО ОБРАБОТКИ ДАННЫХ')\n",
    "print('#'*70 + '\\n')\n",
    "\n",
    "df_result, processing_stats = process_scopus_data(\n",
    "    df_scopus,\n",
    "    df_united,\n",
    "    df_departments,\n",
    "    threshold=FUZZY_MATCH_THRESHOLD,\n",
    "    year=YEAR,\n",
    "    title_exclude_keywords=TITLE_EXCLUDE_KEYWORDS\n",
    ")\n",
    "\n",
    "print('\\n' + '#'*70)\n",
    "print('# ОБРАБОТКА ЗАВЕРШЕНА')\n",
    "print('#'*70)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:46.346484Z",
     "start_time": "2025-11-04T13:39:45.311933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# НАЧАЛО ОБРАБОТКИ ДАННЫХ\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "ОСНОВНОЙ PIPELINE ОБРАБОТКИ\n",
      "======================================================================\n",
      "\n",
      "Фильтрация по году: [2025, 2026]\n",
      "  Scopus - всего статей: 1334\n",
      "  Scopus - статей за [2025, 2026]: 1334\n",
      "  United - всего статей: 1684\n",
      "  United - статей за [2025, 2026]: 1271\n",
      "\n",
      "Фильтрация по Title (исключение статей с подстроками):\n",
      "  Подстроки для исключения: ['Correction:', 'Correction to:', 'Erratum to']\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Correction:\n",
      "title_lower: correction: ai-supported spherical fuzzy decision-making for barriers to renewable energy projects in hospitals: comparative country analysis (energy informatics, (2025), 8, 1, (120), 10.1186/s42162-025-00577-7)\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Correction:\n",
      "title_lower: publisher correction: assessing carbon–neutral supercapacitors in renewable energy systems with self-improving agent-based molecular fuzzy intelligent algorithms (scientific reports, (2025), 15, 1, (28234), 10.1038/s41598-025-12924-5)\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Erratum to\n",
      "title_lower: erratum to: information theoretical approach to detecting quantum gravitational corrections (journal of high energy physics, (2025), 2025, 2, (109), 10.1007/jhep02(2025)109)\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Erratum to\n",
      "title_lower: erratum to: impact of loop quantum gravity on gravitational lensing, thermal fluctuations, tidal force and geodesic deviation around a black hole (the european physical journal c, (2025), 85, 6, (694), 10.1140/epjc/s10052-025-14281-z)\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Correction to:\n",
      "title_lower: correction to: control and effect of climate change due to human activities by mathematical modeling approach under fractional operator (modeling earth systems and environment, (2025), 11, 4, (235), 10.1007/s40808-025-02406-y)\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Correction:\n",
      "title_lower: correction: exploring of sns/nb4c3(gqds) as electrode materials for energy storage devices performance evaluation and development opportunities and hydrogen evolution reactions (the european physical journal plus, (2025), 140, 1, (69), 10.1140/epjp/s13360-025-06027-3)\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Erratum to\n",
      "title_lower: erratum to: a study on the geometric configuration and stability of wormholes in symmetric teleparallel gravity influenced by dark energy and rotational velocity profiles (the european physical journal c, (2025), 85, 5, (478), 10.1140/epjc/s10052-025-14177-y)\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Erratum to\n",
      "title_lower: erratum to “heterogeneous panel data model with sharp and smooth changes: testing green growth hypothesis in g7 countries” [innovation and green development 4 3 (2025) 100245] (innovation and green development (2025) 4(3), (s2949753125000426), (10.1016/j.igd.2025.100245))\n",
      "**********************************************************************\n",
      "\n",
      "keyword: Correction to:\n",
      "title_lower: correction to: hierarchical nanostructuring of pcn-222/nise2@pani composites for enhanced electrochemical performance in supercapattery and hydrogen evolution reaction applications (applied physics a, (2025), 131, 3, (201), 10.1007/s00339-025-08308-1)\n",
      "  Scopus - до фильтрации: 1334\n",
      "  Scopus - исключено статей: 9\n",
      "  Scopus - осталось статей: 1325\n",
      "\n",
      "Поиск новых статей (порог fuzzy matching: 95%)...\n",
      "\n",
      "  Обработано: 100/1325 статей\n",
      "  Обработано: 200/1325 статей\n",
      "  Обработано: 300/1325 статей\n",
      "  Обработано: 400/1325 статей\n",
      "  Обработано: 500/1325 статей\n",
      "  Обработано: 600/1325 статей\n",
      "  Обработано: 700/1325 статей\n",
      "  Обработано: 800/1325 статей\n",
      "  Обработано: 900/1325 статей\n",
      "  Обработано: 1000/1325 статей\n",
      "  Обработано: 1100/1325 статей\n",
      "  Обработано: 1200/1325 статей\n",
      "  Обработано: 1300/1325 статей\n",
      "\n",
      "✓ Анализ завершен:\n",
      "  Всего статей в источнике: 1325\n",
      "  Найдено дубликатов: 1257\n",
      "  Новых статей: 68\n",
      "\n",
      "======================================================================\n",
      "Обработка новых статей...\n",
      "======================================================================\n",
      "\n",
      "  Обработано: 10/68 статей\n",
      "  Обработано: 20/68 статей\n",
      "  Обработано: 30/68 статей\n",
      "  Обработано: 40/68 статей\n",
      "  Обработано: 50/68 статей\n",
      "  Обработано: 60/68 статей\n",
      "\n",
      "✓ Обработка завершена!\n",
      "\n",
      "======================================================================\n",
      "СТАТИСТИКА\n",
      "======================================================================\n",
      "  Всего новых статей найдено: 68\n",
      "  Статей с авторами Khazar: 67\n",
      "  Статей без авторов Khazar: 1\n",
      "  Статей с подсветкой департаментов: 67\n",
      "  Итого строк в результате: 67\n",
      "\n",
      "######################################################################\n",
      "# ОБРАБОТКА ЗАВЕРШЕНА\n",
      "######################################################################\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Экспорт в Excel с форматированием"
  },
  {
   "cell_type": "code",
   "source": "def export_to_excel_with_highlighting(df, output_path, highlight_color='FFFF00'):\n    \"\"\"\n    Экспортирует DataFrame в Excel с желтой подсветкой проблемных ячеек.\n    \n    Параметры:\n    - df: DataFrame с данными (должен содержать колонки _highlight и _highlight_reason)\n    - output_path: путь к выходному файлу\n    - highlight_color: цвет подсветки в формате RGB hex (по умолчанию желтый)\n    \"\"\"\n    \n    if len(df) == 0:\n        print('⚠ DataFrame пустой, нечего экспортировать.')\n        return\n    \n    # Создаем копию без служебных колонок\n    df_export = df.drop(columns=['_highlight', '_highlight_reason'], errors='ignore').copy()\n    \n    # Экспортируем в Excel\n    print(f'Экспорт в файл: {output_path}')\n    df_export.to_excel(output_path, index=False, engine='openpyxl')\n    print(f'  ✓ Данные записаны ({len(df_export)} строк)')\n    \n    # Открываем файл для форматирования\n    wb = openpyxl.load_workbook(output_path)\n    ws = wb.active\n    \n    # Определяем желтый цвет для подсветки\n    yellow_fill = PatternFill(start_color=highlight_color, end_color=highlight_color, fill_type='solid')\n    \n    # Применяем подсветку\n    highlighted_count = 0\n    for idx, row in df.iterrows():\n        if row.get('_highlight', False):\n            # Индекс строки в Excel (учитываем заголовок)\n            excel_row_idx = idx + 2  # +1 для заголовка, +1 для индексации с 0\n            \n            # Подсвечиваем ячейку в колонке Departament (колонка A, индекс 1)\n            cell = ws.cell(row=excel_row_idx, column=1)\n            cell.fill = yellow_fill\n            highlighted_count += 1\n    \n    # Сохраняем\n    wb.save(output_path)\n    print(f'  ✓ Применено форматирование ({highlighted_count} ячеек подсвечено)')\n    print(f'\\n✓ Файл успешно создан: {output_path}')\n\n\n# Проверяем, есть ли результаты для экспорта\nif len(df_result) > 0:\n    # Формируем имя файла с текущей датой и годом/годами\n    current_date = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n    \n    # Формируем строку года для имени файла\n    if YEAR is None:\n        year_str = 'all_years'\n    elif isinstance(YEAR, int):\n        year_str = str(YEAR)\n    else:  # список годов\n        year_str = '-'.join(map(str, sorted(YEAR)))\n    \n    output_filename = OUTPUT_FILE.replace('{year}', year_str).replace('{date}', current_date)\n    \n    print('='*70)\n    print('ЭКСПОРТ РЕЗУЛЬТАТОВ В EXCEL')\n    print('='*70 + '\\n')\n    \n    # Экспортируем\n    export_to_excel_with_highlighting(\n        df_result,\n        output_filename,\n        highlight_color=HIGHLIGHT_COLOR_MULTIPLE_DEPTS\n    )\n    \n    # Дополнительная информация\n    print(f'\\n' + '='*70)\n    print('ИНФОРМАЦИЯ О ПОДСВЕЧЕННЫХ ЯЧЕЙКАХ')\n    print('='*70)\n    \n    # Подсчет причин подсветки\n    highlight_reasons = df_result[df_result['_highlight'] == True]['_highlight_reason'].value_counts()\n    print(f'\\nПричины подсветки:')\n    for reason, count in highlight_reasons.items():\n        reason_text = {\n            'not_found': 'Автор не найден в справочнике',\n            'multiple': 'Множественные департаменты'\n        }.get(reason, reason)\n        print(f'  {reason_text}: {count} статей')\n    \n    print(f'\\n📄 Откройте файл для просмотра:')\n    print(f'   {output_filename}')\n    print(f'\\nЖелтая подсветка означает, что нужно вручную проверить департамент.')\n    \nelse:\n    print('⚠ Нет результатов для экспорта (df_result пустой)')",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T13:39:46.391490Z",
     "start_time": "2025-11-04T13:39:46.354764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ЭКСПОРТ РЕЗУЛЬТАТОВ В EXCEL\n",
      "======================================================================\n",
      "\n",
      "Экспорт в файл: data/output/new_articles_2025-2026_2025-11-04-16-39-46.xlsx\n",
      "  ✓ Данные записаны (67 строк)\n",
      "  ✓ Применено форматирование (67 ячеек подсвечено)\n",
      "\n",
      "✓ Файл успешно создан: data/output/new_articles_2025-2026_2025-11-04-16-39-46.xlsx\n",
      "\n",
      "======================================================================\n",
      "ИНФОРМАЦИЯ О ПОДСВЕЧЕННЫХ ЯЧЕЙКАХ\n",
      "======================================================================\n",
      "\n",
      "Причины подсветки:\n",
      "  Автор не найден в справочнике: 67 статей\n",
      "\n",
      "📄 Откройте файл для просмотра:\n",
      "   data/output/new_articles_2025-2026_2025-11-04-16-39-46.xlsx\n",
      "\n",
      "Желтая подсветка означает, что нужно вручную проверить департамент.\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
